---
layout: post
title: Generative Adversarial Networks (GANs)
github: https://github.com/BenoitLeguay/GAN_IconClass
---

Curious about GANs framework, I wanted to deal with it in depth. I completed the online course ([available on Coursera](https://www.coursera.org/specializations/generative-adversarial-networks-gans)), and then I decided to implement most of framework versions (vanilla DCGANs, WGANs, Conditional GANs etc...)

GANs are powerful architecture that can learn to generate data with same statistics of a given set. They have many applications: generate unreal human faces based on true ones, up-scale image or video resolution, create DNA sequences with certain properties,  etc... 

The main idea behind GAN is to use a *Generator* that creates fake examples and a *Discriminator* that tries to find these fake examples among the reals. Through an iterative process, it's likely that the generator creates more accurate example, and the discriminator classifies better fake and reals examples. Both the *Discriminator* and the *Generator* are neural networks. This duel between our 2 networks involves a *Game Theory* component, that we'll discuss later on. 



We will start with a very basic example, using a simple GAN architecture and dataset.

### Vanilla GAN

**Dataset**

Let's start with the dataset, I generate a population from a Gaussian Multivariate distribution with random mean and covariance. For visualization purpose, I make this example with both 2-dimensional and 3-dimensional data. 

![gans-real.png]({{site.baseurl}}/images/gans/gans-real-2d.png)

*Real examples (2-dimensional)*

![gans-real.png]({{site.baseurl}}/images/gans/gans-real-3d.png)

*Real examples (3-dimensional)*

About the architecture, we've got 2 fully connected neural networks with 2 hidden layers. 

**GANs architecture**

*Generator*

The generator takes a random vector as input of size *z* and outputs a vector having the same size as $$x_i$$. 

*Discriminator*

The discriminator takes a input shape vector (real and fake) and output a classification score. 



Thanks to this example, we can easily observe and compare our real and fake data. Here below we have the evolution of the fake generated during the training process:

![gans-fake-real.gif]({{site.baseurl}}/images/gans/gans-fake-real.gif)

*Generated fake along with real examples regarding epochs*

GANs learn the distribution statistics and can output almost real example with a quit good variety. Obviously the goal is to produce diversified example and to prevent mode collapse.  

A **mode collapse** refers to a generator model that is only capable of generating one or a small subset of different outcomes, or modes. The output resembles a real one, the discriminator is fooled, but the Generator can only imitate this little area of the distribution. 

![gans-mode_collapse.png]({{site.baseurl}}/images/gans/gans-mode_collapse.png)

*Example of a Generator suffering from mode collapse*



An interesting parameter to play with in this example is the dimension of the Z input. Z is the noise vector, a random normal generated vector that is the *Generator's* input. The bigger the dimension of Z, the more information is contained in it and thus easier it is for the *Generator* to achieve good diversity. Let's compare 2 training process with Z having different lengths.  

![gans-zdim-1.gif]({{site.baseurl}}/images/gans/gans-zdim-1.gif)

*Z dimension: 1*

![gans-zdim-100.gif]({{site.baseurl}}/images/gans/gans-zdim-100.gif)

*Z dimension: 100*

On one side, the generated population lacks diversity, GANs do well inferring the overall trend of the data but is unable to capture the variance on another dimension. On the other side, our GANs reach a better variety but are more biased toward the training set.